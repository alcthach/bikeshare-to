[[2023-01-23]]

---

09:20


	## Closing Up for the Day
	- Bit of a review with python, was nice to write a quick and dirty script
	- Re-familiarized myself with data types in python 
	- Was nice to just be in a REPL environment working with python 
	- Overall, I expect to complete the task of loading station data to the database during my next session
	- The itertools chain module was really cool to employ; and save me a ton of time
	- I'll look to re-initialize the `stations` table with the missing fields, then impute null in the missing data in the copy script that I'll use after

From 
[[bike-share-dev-log-2023-01-19]]

Looks like I'll be re-initializing the table to include the fields I was missing. Then I could try to load in the missing data.

Something I'll have to watch out for is how nulls are going to be handled. 

I'll also have to watch out for column order but I expect that the order of the fields are standardized.

It looks like order is the same for each element however I'm not sure if that actually matters. It'll depend on the command I use and how the sql engine handles the loading of the csv file onto the database. I'll have to review the docs a bit.

Reading the docs...

I'm a bit confused. I think I'll have to take a look at the csv file itself as I might be required to transform the data at that level. I'm not sure how the `csv` module handled the missing keys in some of the elements in the `stations` object.

I'll investigate further.

I.E. Review the code I used to convert the json object to a csv

``` python
def main():
    count = 0
    
    station_data = get_station_data()

    for station in station_data:
        if count == 0:

            header = station.keys()
            csv_writer.writerow(header)
            count += 1

        csv_writer.writerow(station.values())

    data_file.close()
```

``` pseudocode
define function called main
	initialize counter with starting value 0

	get my data object from the API endpoint and store in a var

	initialize for loop
		init if counter is 0
		hmmm...

   I think the if loop here is used to write the header of the csv file.
		
```

`count` would only be incremented once. To ensure that the header can be written to the csv file. This assumes that the set of keys for each element in `stations` is the same. Otherwise I would have something weird going on in my csv file. Which there's a chance that I do at this point. I can't even load the data to the sql database right now.

Probably best for me to print some outputs in the code just to see what's going on.

My thoughts exactly.

``` python
[athach@fedora stations]$ python main.py
dict_keys(['station_id', 'name', 'physical_configuration', 'lat', 'lon', 'altitude', 'address', 'capacity', 'is_charging_station', 'rental_methods', 'groups', 'obcn', 'nearby_distance', '_ride_code_support', 'rental_uris'])
[athach@fedora stations]$
```

It pulled the keys from the first element in `stations`.

I might have to pass a list of keys explicitly. However, I'm a bit concerned with how the module is going to handle the missing key-value pairs. 

Time to read the docs again.

From what I understand, there is only `csv.writerow` in terms of writing a line to the csv file. `header` was an argument that I passed into `csvwriter.writerows()` `header` was the list of keys used to write the header for the csv file.  

I could pass through a dynamic list using the pattern that I learned the other day.

Instead of using the first element in `stations` I could use the list generated by the `UniqueKeys()` function instead.

I got the correct headers but it's in the wrong order. The appears to be that the `csvwriter.writerows()` function writes the string to the file indiscriminately. I might need to refer to the work that I did on the FPL project. I think the pattern I employed there might have been more robust.

## Wrapping Up
- Need to figure out a way to load the records despite them not having homogeneous sets of keys
- 